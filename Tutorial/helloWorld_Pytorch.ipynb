{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### xにとってｙを一番よく説明すること変数ｗ、ｂを探す  \n",
    "この場合どのくらい予測が正確なのか判断するmetric \"MSE\"を使います  \n",
    "MSE = Mean Squared Error  \n",
    "\n",
    "従って、\n",
    "$$Loss = {(y^*-y)^2 \\over n}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lossをminimizeするw、bを探したい。\n",
    "\n",
    "Random Search (X)  \n",
    "Gradient Descent (O)\n",
    "\n",
    "$$ w_{t+1} = w_t - gradient * learning\\_rate $$  \n",
    "どんどんgradientが低い方向に向かって進む\n",
    "\n",
    "SGD, Adam , Optimizer, Adagrad, SparseAdam... いろんな種類のOptimizerがある。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layerが1存在する場合  \n",
    "$$ y = w2(act(w1 * input + b1)) + b2 $$\n",
    "\n",
    "#### Layerが3存在する場合  \n",
    "$$ y = w4(act(w3(act(w2(act(w1*input+b1))+b2))+b3))+b4 $$\n",
    "\n",
    "Activation functionでnon-linearlityを追加する  \n",
    "sigmoid, tanH, ReLU等がある"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Propagation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward & Back Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ forward = y^* = w3*act(w2*act(w1*act+b1)+b2)+b3$$\n",
    "$$ loss = y^*-y = w3*act(w2*act(w1*x+b1)+b2)+b3-y$$\n",
    "$$ \\frac {\\partial loss}{\\partial w3} = act(w2*act(w1*x+b1)b2) $$\n",
    "$$ \\frac {\\partial loss}{\\partial b3} = 1 $$\n",
    "$$ \\frac {\\partial loss}{\\partial w2} = chain Rule!! $$\n",
    "$$ Chain Rule = \\frac {\\partial z}{\\partial x} = \\frac {\\partial z}{\\partial y}\\frac {\\partial y}{\\partial x} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# practice\n",
    "\n",
    "## Workflow\n",
    "### Data > Model > Ouput > Loss > Gradients > Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1)Declaration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_tensor = torch.Tensor(3,4)\n",
    "x_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3733e-14, 6.4076e+07, 2.0706e-19, 7.3909e+22],\n",
       "        [2.4176e-12, 1.1625e+33, 8.9605e-01, 1.1632e+33],\n",
       "        [5.6003e-02, 7.0374e+22, 5.7453e-44, 0.0000e+00]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable = Variable(x_tensor)\n",
    "x_variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Variables of a Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.3733e-14, 6.4076e+07, 2.0706e-19, 7.3909e+22],\n",
       "        [2.4176e-12, 1.1625e+33, 8.9605e-01, 1.1632e+33],\n",
       "        [5.6003e-02, 7.0374e+22, 5.7453e-44, 0.0000e+00]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "print(x_variable.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(x_variable.requires_grad)\n",
    "\n",
    "x_variable = Variable(x_tensor, requires_grad=True)\n",
    "x_variable.requires_grad\n",
    "\n",
    "# requires_grad : この変数についてgradientの演算をするかどうかを決定する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Users\\LSH\\Anaconda3\\envs\\cuda\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "D:\\Users\\LSH\\Anaconda3\\envs\\cuda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: volatile was removed (Variable.volatile is always False)\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, False, False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_variable = Variable(x_tensor, volatile=True)\n",
    "x_variable.grad, x_variable.requires_grad, x_variable.volatile\n",
    "\n",
    "# volatile = minimal memory usage.\n",
    "# 一つのvolatileが全てのgraphのgradient必要がなくなる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Graph & Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, True, True)"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = Variable(torch.FloatTensor(3,4), requires_grad=True)\n",
    "y = x**2 + 4*x\n",
    "z = 2*y+3\n",
    "\n",
    "x.requires_grad, y.requires_grad, z.requires_grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss  = torch.FloatTensor(3,4)\n",
    "z.backward(loss)\n",
    "\n",
    "print(x.grad)\n",
    "y.grad, z.grad"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Pytorch",
   "language": "python",
   "name": "cuda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
